{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"machine_shape":"hm","gpuType":"T4","cell_execution_strategy":"setup","authorship_tag":"ABX9TyPvFu2faQXUgKPT21x7T4gx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install datasets\n","# !pip install torchvision\n","# !pip install torch transformers"],"metadata":{"id":"sbhCc2iasHMy","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","from google.colab import drive\n","from datasets import load_dataset, load_from_disk\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import warnings\n","\n","# !wget -O vqgan_imagenet_f16_16384.yaml https://heibox.uni-heidelberg.de/f/140747ba53464f318b3d/?dl=1\n","# !wget -O vqgan_imagenet_f16_16384.ckpt https://heibox.uni-heidelberg.de/f/13e75467430e4b98a1ff/?dl=1\n","\n","# Ignore specific warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/CS224N_project/\n","dataset = load_from_disk('/content/drive/MyDrive/CS224N_project/MagicBrush')\n","!git clone https://github.com/xinntao/ESRGAN.git\n","# %cd ESRGAN\n","sys.path.append('/content/drive/MyDrive/CS224N_project/')\n","sys.path.append('/content/drive/MyDrive/CS224N_project/ESRGAN')"],"metadata":{"id":"9jluWdAesepO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHYTA9kWope_"},"outputs":[],"source":["first_image_data = dataset['train'][3]\n","source_img = first_image_data['source_img']\n","mask_img = first_image_data['mask_img']\n","target_img = first_image_data['target_img']\n","\n","fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","axs[0].imshow(source_img)\n","axs[0].set_title('Source Image')\n","axs[0].axis('off')\n","\n","axs[1].imshow(np.array(mask_img)[:, :, -1])\n","axs[1].set_title('Mask Image')\n","axs[1].axis('off')\n","\n","axs[2].imshow(target_img)\n","axs[2].set_title('Target Image')\n","axs[2].axis('off')\n","\n","plt.show()\n","\n","print(first_image_data['instruction'])"]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2\n","# from MMF import MMF\n","from MMF_mask import MMF_mask\n","from MMF_mask_CLIP import MMF_mask_CLIP\n","from MMF_mask_cat import MMF_mask_cat\n","from MMF_mask_multihead import MMF_mask_multihead\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from PIL import Image\n","from torchvision import transforms\n","from Preprocess_mask import MagicBrushDataset\n","# from MMF_transform import MultimodalTransformer\n","from torch.utils.data import DataLoader, Dataset, Subset, random_split\n","from Image_printer import visualize_results\n","from PatchDiscriminator import PatchDiscriminator\n","from torchvision.utils import save_image\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","batch_size = 4\n","\n","dataset_train = MagicBrushDataset(dataset['train'])\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","dataset_dev = MagicBrushDataset(dataset['dev'])\n","dataloader_dev = DataLoader(dataset_dev, batch_size=batch_size, shuffle=True)\n","\n","dataset_size = len(dataset_train)\n","train_size = int(0.2 * dataset_size)\n","eval_size = int(0.02 * dataset_size)\n","test_size = dataset_size - train_size - eval_size\n","train_dataset, eval_dataset, test_dataset = random_split(dataset_train, [train_size, eval_size, test_size])\n","\n","dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","dataloader_eval = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n","dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","dataset_size = len(dataset_train)\n","indices = np.arange(dataset_size)\n","np.random.shuffle(indices)\n","subset_indices = indices[:int(0.2 * dataset_size)]\n","subset_indices = list(map(int, subset_indices))\n","\n","subset_dataset = Subset(dataset_train, subset_indices)\n","dataloader_subset = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)\n","subset_dataset = Subset(dataset_dev, subset_indices)\n","dataloader_subset_dev = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)\n"],"metadata":{"id":"kCZnUjxJt67U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = MMF_mask_cat().to(device)\n","model_attention = MMF_mask().to(device)\n","model_CLIP = MMF_mask_CLIP().to(device)\n","model_multihead = MMF_mask_multihead().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_attention = torch.optim.Adam(model_attention.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_attention_multihead = torch.optim.Adam(model_multihead.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_CLIP = torch.optim.Adam(model_CLIP.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_attention, mode='min', factor=0.5, patience=3)\n","criterion = nn.BCELoss()\n","\n","save_path = '/content/drive/MyDrive/CS224N_project/MMF_mask_cat.pth'########## need to change\n","save_path_attention = '/content/drive/MyDrive/CS224N_project/MMF_model_cross_attention.pth'\n","save_path_attention_multihead = '/content/drive/MyDrive/CS224N_project/MMF_model_cross_attention_multihead.pth'\n","save_path_CLIP = '/content/drive/MyDrive/CS224N_project/MMF_model_CLIP.pth'\n","\n","# model.load_state_dict(torch.load(save_path), strict=False)\n","# model_attention.load_state_dict(torch.load(save_path_attention), strict=False)\n","# model_CLIP.load_state_dict(torch.load(save_path_CLIP), strict=False)\n","# MMF_mask_multihead.load_state_dict(torch.load(save_path_attention_multihead), strict=False)"],"metadata":{"id":"6tcnmyOlew8S","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scores_cat = [] ########## Cat #############\n","# import matplotlib.pyplot as plt\n","# num_epochs = 50\n","# for epoch in range(num_epochs):\n","#     model.train()\n","#     scores = []\n","#     for source_img, target_img, instruction in tqdm(dataloader_train, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n","#         source_img = source_img.to(device)\n","#         target_img = target_img.to(device)\n","#         # Forward pass\n","#         prediction = model(source_img, instruction)\n","#         # prediction = (prediction > 0.9).float()\n","#         # Calculate loss\n","#         loss = criterion(prediction, target_img)\n","#         # Backward pass and optimization\n","#         optimizer.zero_grad()\n","#         loss.backward()\n","#         optimizer.step()\n","#         scores.append(loss.item())\n","#     print(f'Epoch {epoch+1}/{num_epochs}, Avg Loss: {np.mean(scores):.4f}')\n","#     scores_cat.append(np.mean(scores))\n","#     torch.save(model.state_dict(), save_path)"],"metadata":{"id":"9Wd8-37UcWnC","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scores_CLIP = [] ########## CLIP ###################\n","# import matplotlib.pyplot as plt\n","# num_epochs = 50\n","# best_score = float('inf')\n","# for epoch in range(num_epochs):\n","#     model.train()\n","#     scores = []\n","#     for source_img, target_img, instruction in tqdm(dataloader_train, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n","#         source_img = source_img.to(device)\n","#         target_img = target_img.to(device)\n","#         # Forward pass\n","#         prediction = model_CLIP(source_img, instruction)\n","#         # prediction = (prediction > 0.9).float()\n","#         # Calculate loss\n","#         loss_CLIP = criterion(prediction, target_img)\n","#         # Backward pass and optimization\n","#         optimizer_CLIP.zero_grad()\n","#         loss_CLIP.backward()\n","#         optimizer_CLIP.step()\n","#         scores.append(loss_CLIP.item())\n","#     print(f'Epoch {epoch+1}/{num_epochs}, Avg Loss: {np.mean(scores):.4f}')\n","#     torch.save(model.state_dict(), save_path_CLIP)\n","#     scores_CLIP.append(np.mean(scores))"],"metadata":{"id":"tMVaiYvf3Y2e","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scores_attention = [] ########## Attention #############\n","# import matplotlib.pyplot as plt\n","# num_epochs = 50\n","# best_score = float('inf')\n","# for epoch in range(num_epochs):\n","#     model_attention.train()\n","#     scores = []\n","#     for source_img, target_img, instruction in tqdm(dataloader_train, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n","#         source_img = source_img.to(device)\n","#         target_img = target_img.to(device)\n","#         # Forward pass\n","#         prediction = model_attention(source_img, instruction)\n","#         # prediction = (prediction > 0.9).float()\n","#         # Calculate loss\n","#         loss_attention = criterion(prediction, target_img)\n","#         # Backward pass and optimization\n","#         optimizer_attention.zero_grad()\n","#         loss_attention.backward()\n","#         optimizer_attention.step()\n","#         scores.append(loss_attention.item())\n","#     print(f'Epoch {epoch+1}/{num_epochs}, Avg Loss: {np.mean(scores):.4f}')\n","#     torch.save(model_attention.state_dict(), save_path_attention)\n","#     scores_attention.append(np.mean(scores))\n"],"metadata":{"id":"xwtHFzKl3X01","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores_attention_multihead = [] ########## Attention #############\n","import matplotlib.pyplot as plt\n","num_epochs = 50\n","best_score = float('inf')\n","for epoch in range(num_epochs):\n","    # MMF_mask_multihead.train()\n","    scores = []\n","    for source_img, target_img, instruction in tqdm(dataloader_train, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n","        source_img = source_img.to(device)\n","        target_img = target_img.to(device)\n","        # Forward pass\n","        prediction = model_multihead(source_img, instruction)\n","        # prediction = (prediction > 0.9).float()\n","        # Calculate loss\n","        loss_attention_multihead = criterion(prediction, target_img)\n","        # Backward pass and optimization\n","        optimizer_attention_multihead.zero_grad()\n","        loss_attention_multihead.backward()\n","        optimizer_attention_multihead.step()\n","        scores.append(loss_attention_multihead.item())\n","    print(f'Epoch {epoch+1}/{num_epochs}, Avg Loss: {np.mean(scores):.4f}')\n","    torch.save(model_multihead.state_dict(), save_path_attention)\n","    scores_attention_multihead.append(np.mean(scores))"],"metadata":{"id":"DR1apMD1j2cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","model_attention.eval()\n","model_CLIP.eval()\n","MMF_mask_multihead.eval()\n","# with torch.no_grad():\n","#     scores = []\n","#     for source_img, target_img, instruction in tqdm(dataloader_eval, desc='Evaluating'):\n","#         source_img = source_img.to(device)\n","#         target_img = target_img.to(device)\n","\n","#         outputs = model(source_img, instruction)\n","#         loss = criterion(outputs, target_img)\n","#         scores.append(loss.item())\n","#     print(f'Concatenation evaluation Avg Loss: {np.mean(scores):.4f}')\n","# with torch.no_grad():\n","#     scores = []\n","#     for source_img, target_img, instruction in tqdm(dataloader_eval, desc='Evaluating'):\n","#         source_img = source_img.to(device)\n","#         target_img = target_img.to(device)\n","\n","#         outputs = model_attention(source_img, instruction)\n","#         loss = criterion(outputs, target_img)\n","#         scores.append(loss.item())\n","#     print(f'cross attention evaluation Avg Loss: {np.mean(scores):.4f}')\n","# with torch.no_grad():\n","#     scores = []\n","#     for source_img, target_img, instruction in tqdm(dataloader_eval, desc='Evaluating'):\n","#         source_img = source_img.to(device)\n","#         target_img = target_img.to(device)\n","\n","#         outputs = model_CLIP(source_img, instruction)\n","#         loss = criterion(outputs, target_img)\n","#         scores.append(loss.item())\n","#     print(f'CLIP evaluation Avg Loss: {np.mean(scores):.4f}')\n","with torch.no_grad():\n","    scores = []\n","    for source_img, target_img, instruction in tqdm(dataloader_eval, desc='Evaluating'):\n","        source_img = source_img.to(device)\n","        target_img = target_img.to(device)\n","\n","        outputs = MMF_mask_multihead(source_img, instruction)\n","        loss = criterion(outputs, target_img)\n","        scores.append(loss.item())\n","    print(f'CLIP evaluation Avg Loss: {np.mean(scores):.4f}')"],"metadata":{"id":"K0fU86rYxMnl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","with torch.no_grad():\n","        for source_img, target_img, instruction in dataloader_eval:\n","            source_img = source_img.to(device)\n","            target_img = target_img.to(device)\n","\n","            outputs = model(source_img, instruction)\n","            outputs = (outputs > 0.2).float()\n","            visualize_results(source_img, target_img, outputs, instruction)\n","            outputs_attention = model_attention(source_img, instruction)\n","            outputs_attention = (outputs_attention > 0.25).float()\n","            visualize_results(source_img, target_img, outputs_attention, instruction)\n","            outputs_CLIP = model_CLIP(source_img, instruction)\n","            outputs_CLIP = (outputs_CLIP > 0.2).float()\n","            outputs_attention = model_multihead(source_img, instruction)\n","            outputs_attention = (outputs_attention > 0.2).float()\n","            visualize_results(source_img, target_img, outputs_CLIP, instruction)\n","            break"],"metadata":{"id":"q20Ecv8y-7-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# with open('scores_cat.pkl', 'wb') as file:########## need to change\n","# #     pickle.dump(scores_cat, file)########## need to change\n","# with open('scores_attention.pkl', 'wb') as file:########## need to change\n","#     pickle.dump(scores_attention, file)########## need to change\n","# with open('scores_CLIP.pkl', 'wb') as file:########## need to change\n","    # pickle.dump(scores_CLIP, file)########## need to change\n","with open('scores_attention_multihead.pkl', 'wb') as file:########## need to change\n","    pickle.dump(scores_attention_multihead, file)########## need to change"],"metadata":{"id":"Rpahi42XOFRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# with open('scores_cat.pkl', 'rb') as file:\n","#     scores_cat = pickle.load(file)\n","# with open('scores_attention.pkl', 'rb') as file:\n","#     scores_attention = pickle.load(file)\n","# with open('scores_CLIP.pkl', 'rb') as file:\n","#     scores_CLIP = pickle.load(file)\n","\n","import matplotlib.pyplot as plt\n","\n","plt.plot(scores_attention_multihead, label = 'sssd')\n","plt.xlabel('epoch')\n","plt.ylabel('Score')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ee_oS_zKwJgh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gkuKg3BSP3_g"},"execution_count":null,"outputs":[]}]}